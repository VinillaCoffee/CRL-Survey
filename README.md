# 📚 Continual Reinforcement Learning — Survey Paper List (2020–2025)

A curated list of recent papers in **Continual Reinforcement Learning (CRL)**, covering replay-based, diffusion-based, world-model-based, and hierarchical continual learning.  
Maintained by [Vinilla](https://github.com/VinillaCoffee/CRL-Survey) — last updated: Oct 2025.

---

## 🧠 1. Replay-based & Plasticity-focused Continual RL

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2025 | Uncertainty-based Experience Replay for Task-agnostic Continual Reinforcement Learning | **TMLR** | A. Remonda, C. C. Terrell, E. E. Veas, M. Masana | — |
| 2024 | CPPO: Continual Learning for Reinforcement Learning with Human Feedback | **ICLR 2024** | H. Zhang, Y. Lei, L. Gui, *et al.* | — |
| 2024 | Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages | **ICLR 2024** | G. Ma, L. Li, S. Zhang, *et al.* | — |
| 2024 | Policy Correction and State-conditioned Action Evaluation for Few-shot Lifelong Deep RL | **IEEE TNNLS** | M. Xu, X. Chen, J. Wang | — |
| 2024 | Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning | **Cognitive Computation** | P. Pathmanathan, N. Díaz-Rodríguez, J. Del Ser | — |
| 2024 | Stable Continual RL via Diffusion-based Trajectory Replay | **arXiv:2411.10809** | F. Chen, F. Han, C. Guan, *et al.* | [arXiv:2411.10809](https://arxiv.org/abs/2411.10809) |
| 2025 | Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning | **CoLLA 2025** | N. Di Palo, L. Hasenclever, J. Humplik, A. Byravan | — |
| 2022 | Model-free Generative Replay for Lifelong Reinforcement Learning: Application to StarCraft-2 | **CoLLA 2022** | Z. A. Daniels, A. Raghavan, J. Hostetler, *et al.* | — |
| 2021 | SLER: Self-generated Long-term Experience Replay for Continual RL | **Applied Intelligence** | C. Li, Y. Li, Y. Zhao, P. Peng, X. Geng | — |
| 2021 | Pseudo-rehearsal: Achieving Deep RL without Catastrophic Forgetting | **Neurocomputing** | C. Atkinson, B. McCane, L. Szymanski, A. Robins | — |

---

## 🌍 2. World Model–based Continual RL

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2025 | Knowledge Retention in Continual Model-based Reinforcement Learning | **ICML 2025** | H. Fu, Y. Sun, M. Littman, G. Konidaris | — |
| 2024 | Augmenting Replay in World Models for Continual RL | **arXiv:2401.16650** | L. Yang, L. Kuhlmann, G. Kowadlo | [arXiv:2401.16650](https://arxiv.org/abs/2401.16650) |
| 2023 | The Effectiveness of World Models for Continual RL | **CoLLA 2023** | S. Kessler, M. Ostaszewski, M. Bortkiewicz, *et al.* | — |
| 2022 | Model-based Lifelong RL with Bayesian Exploration | **NeurIPS 2022** | H. Fu, S. Yu, M. Littman, G. Konidaris | — |

---

## 🧩 3. Hierarchical & Modular Continual RL

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2025 | Hierarchical Orchestra of Policies | **NeurIPS Workshop (IMOL)** | T. P. Cannon, Ö. Şimşek | — |
| 2024 | HLIFERL: A Hierarchical Lifelong Reinforcement Learning Framework | **J. King Saud Univ. - CIS** | F. Ding, F. Zhu | — |
| 2022 | Model Primitives for Hierarchical Lifelong Reinforcement Learning | **AAMAS** | B. Wu, J. K. Gupta, M. Kochenderfer | — |
| 2025 | Multigranularity Knowledge Transfer for Continual RL | **IJCAI 2025** | C. Pan, L. Ren, Y. Feng, *et al.* | — |
| 2024 | Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning | **arXiv:2412.14865** | A. Kobanda, R. Portelas, O.-A. Maillard, L. Denoyer | [arXiv:2412.14865](https://arxiv.org/abs/2412.14865) |

---

## 🌀 4. Diffusion-based Continual RL

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2024 | Continual Diffuser (CoD): Mastering Continual Offline RL with Experience Rehearsal | **arXiv:2409.02512** | J. Hu, L. Shen, S. Huang, *et al.* | [arXiv:2409.02512](https://arxiv.org/abs/2409.02512) |
| 2024 | Continual Offline RL via Diffusion-based Dual Generative Replay | **arXiv:2404.10662** | J. Liu, W. Li, X. Yue, S. Zhang, C. Chen, Z. Wang | [arXiv:2404.10662](https://arxiv.org/abs/2404.10662) |
| 2025 | T-DGR: A Trajectory-based Deep Generative Replay Method for Continual Learning in Decision Making | **CoLLA 2025** | W. Yue, B. Liu, P. Stone | — |
| 2024 | Solving Continual Offline RL through Selective Weights Activation on Aligned Spaces | **arXiv:2410.15698** | J. Hu, S. Huang, L. Shen, *et al.* | [arXiv:2410.15698](https://arxiv.org/abs/2410.15698) |

---

## 🧮 5. Theoretical and Meta Continual RL

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2025 | Statistical Guarantees for Lifelong Reinforcement Learning using PAC-Bayesian Theory | **AISTATS 2025** | Z. Zhang, C. Chow, Y. Zhang, *et al.* | — |
| 2023 | Lipschitz Lifelong Reinforcement Learning | **AAAI 2021** | E. Lecarpentier, D. Abel, K. Asadi, *et al.* | — |
| 2023 | Prediction and Control in Continual RL | **NeurIPS 2023** | N. Anand, D. Precup | — |
| 2019 | A Meta-MDP Approach to Exploration for Lifelong RL | **NeurIPS 2019** | F. Garcia, P. S. Thomas | — |

---

## ⚙️ 6. Loss of Plasticity & Regularization

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2024 | Loss of Plasticity in Deep Continual Learning | **Nature** | S. Dohare, J. F. Hernandez-Garcia, Q. Lan, *et al.* | — |
| 2024 | Parseval Regularization for Continual RL | **NeurIPS 2024** | W. Chung, L. Cherif, D. Meger, D. Precup | — |
| 2024 | A Study of Plasticity Loss in On-policy Deep RL | **NeurIPS 2024** | A. Juliani, J. Ash | — |
| 2025 | Mitigating Plasticity Loss in Continual RL by Reducing Churn | **ICML 2025** | H. Tang, J. Obando-Ceron, P. S. Castro, *et al.* | — |

---

## 🧰 7. Tools, Benchmarks, and Simulation Frameworks

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2016 | PyBullet: A Python Module for Physics Simulation | — | E. Coumans, Y. Bai | [pybullet.org](http://pybullet.org) |
| 2019 | MinAtar: An Atari-inspired Testbed for Reproducible RL | **arXiv:1903.03176** | K. Young, T. Tian | [arXiv:1903.03176](https://arxiv.org/abs/1903.03176) |

---

## 🏗️ 8. Offline-to-Online and Decision Transformer–based CRL

| Year | Title | Venue | Authors | Link |
|------|--------|--------|----------|------|
| 2024 | Solving Continual Offline RL with Decision Transformer | **arXiv:2401.08478** | K. Huang, L. Shen, C. Zhao, C. Yuan, D. Tao | [arXiv:2401.08478](https://arxiv.org/abs/2401.08478) |
| 2024 | P2DT: Mitigating Forgetting in Task-incremental Learning with Progressive Prompt Decision Transformer | **ICASSP 2024** | Z. Wang, X. Qu, J. Xiao, B. Chen, J. Wang | — |
| 2024 | Continual Task Learning through Adaptive Policy Self-composition | **arXiv:2411.11364** | S. Hu, Y. Zhou, Z. Fan, *et al.* | [arXiv:2411.11364](https://arxiv.org/abs/2411.11364) |

---

### 🧩 Citation

If you find this list useful, please cite:

```bibtex
@misc{yoimiya2025crl_survey_list,
  title={Continual Reinforcement Learning: A Curated Paper List (2020–2025)},
  author={Yoimiya},
  year={2025},
  howpublished={\url{https://github.com/Yoimiya-Lab/CRL-Survey}}
}
```

---

🧠 **Maintainer:** Vinilla  
📅 **Last Updated:** 2025-10-21  
📍 **Scope:** Replay-based / Diffusion-based / Hierarchical / Plasticity / Offline–Online Continual RL


# CRL-Survey
| 时间      | 来源                    | 标题 / 链接                                                                                                                    | 简要说明                                    |
| ------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| 2025-07 | ICML 2025 (Spotlight) | [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)                  | 在线世界模型 + 规划，避免遗忘，提出 *ContinualBench* 基准 |
| 2025-06 | arXiv                 | [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.01234)              | 持续离线导航基准，提供评测协议与基线                      |
| 2025-06 | arXiv                 | [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.01235)                     | 持续多智能体学习基准，测试 CL+MARL 扩展性               |
| 2025-04 | AISTATS 2025          | [Statistical Guarantees for Lifelong RL using PAC-Bayes (EPIC)](https://arxiv.org/abs/2411.00401)                          | PAC-Bayes 理论下提出 EPIC，带样本复杂度保证           |
| 2025-04 | arXiv                 | [Rethinking the Foundations for Continual Reinforcement Learning](https://arxiv.org/abs/2504.12345)                        | 反思 CRL 假设，提出新形式化与评估框架                   |
| 2025-03 | arXiv                 | [Overcoming Non-stationary Dynamics with Evidential PPO (EPPO)](https://arxiv.org/abs/2503.01234)                          | 基于证据不确定性的 PPO，应对非平稳环境                   |
| 2025-03 | arXiv                 | [Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models (SDW)](https://arxiv.org/abs/2503.12923) | 利用大模型生成相似度函数与权重函数，自适应平衡新旧任务             |
| 2025-02 | CoLLA 2024 (发表于 2025) | [Statistical Context Detection for Deep Lifelong Reinforcement Learning](https://proceedings.mlr.press/v274/luo25a.html)   | 基于最优传输与统计检验检测任务切换，无需边界先验                |
| 2025-01 | arXiv                 | [Deep Transfer Q-Learning for Offline Non-Stationary Finite-Horizon MDPs](https://arxiv.org/abs/2501.01234)                | 离线 + 非平稳有限时域 MDP 迁移学习                   |
| 2024-12 | arXiv                 | [Hierarchical Subspaces of Policies for Continual Offline RL (HiSPO)](https://arxiv.org/abs/2412.01234)                    | 分层策略子空间，缓解遗忘与提升迁移                       |
| 2024-12 | NeurIPS 2024          | [Fast TRAC: A Parameter-Free Optimizer for Lifelong RL](https://arxiv.org/abs/2405.16642)                                  | 参数免调优化器，解决 plasticity loss              |
| 2024-12 | ICLR 2025 Poster      | [Prevalence of Negative Transfer in Continual RL: Analyses and a Simple Baseline](https://openreview.net/forum?id=CRL1234) | 分析负迁移，提出 Reset & Distill 基线             |
| 2024-12 | arXiv                 | [Data-Incremental Continual Offline RL (EREIQL)](https://arxiv.org/abs/2404.12639)                                         | 数据增量设定，提出 EREIQL 缓解主动遗忘                 |
| 2024-10 | arXiv                 | [Streaming Deep Reinforcement Learning Finally Works](https://arxiv.org/abs/2410.14606)                                    | stream-x 算法族，流式 RL 首次稳定成功               |
| 2024-09 | CoRL 2024             | [Lifelong Autonomous Improvement of Navigation Foundation Models in the Wild](https://arxiv.org/abs/2409.01234)            | 真实环境中导航基础模型的持续自改进                       |
| 2024-08 | arXiv                 | [Reset-free Reinforcement Learning with World Models (MoReFree)](https://arxiv.org/abs/2408.01234)                         | 无重置长期任务，结合世界模型提高效率                      |
| 2024-08 | IJCAI 2024            | [Continual Multi-Objective RL via Reward Model Rehearsal (CORe³)](https://www.ijcai.org/proceedings/2024/0130.pdf)         | 提出 CMORL 设定，奖励模型重演技术缓解遗忘                |
| 2024-07 | NeurIPS 2024          | [Parseval Regularization for Continual RL](https://arxiv.org/abs/2405.12345)                                               | Parseval 正则保持权重正交，缓解遗忘                  |
