# A Comprehensive Survey of Continual Reinforcement Learning: From Online to Offline

A curated list of recent papers in **CRL**, covering CORL and online CRL.  
Maintained by [Vinilla](https://github.com/VinillaCoffee/CRL-Survey) — last updated: Oct 2025.

---

## Survey Papers
| Published | Title |
|------------|--------|
| 2025 **TMLR** | [Uncertainty-based experience replay for task-agnostic continual reinforcement learning](~) |
| 2024 **ICLR** | [CPPO: Continual learning for reinforcement learning with human feedback](~) |
| 2024 **ICLR** | [Revisiting plasticity in visual reinforcement learning: Data, modules and training stages](~) |
| 2024 **TNNLS** | [Policy correction and state-conditioned action evaluation for few-shot lifelong deep reinforcement learning](~) |
| 2024 **Cognitive Computation** | [Using curiosity for an even representation of tasks in continual offline reinforcement learning](~) |
| 2023 **TMLR** | [Replay-enhanced continual reinforcement learning](~) |
| 2020 **arXiv** | [Continual reinforcement learning with multi-timescale replay](~) |
| 2019 **NeurIPS** | [Experience replay for continual learning](~) |
| 2018 **AAAI** | [Selective experience replay for lifelong learning](~) |
| 2025 **CoLLAs** | [Diffusion augmented agents: A framework for efficient exploration and transfer learning](~) |
| 2024 **arXiv** | [Stable continual reinforcement learning via diffusion-based trajectory replay](~) |
| 2022 **CoLLAs** | [Model-free generative replay for lifelong reinforcement learning: Application to starcraft-2](~) |
| 2021 **Applied Intelligence** | [Sler: Self-generated long-term experience replay for continual reinforcement learning](~) |
| 2021 **Neurocomputing** | [Pseudo-rehearsal: Achieving deep reinforcement learning without catastrophic forgetting](~) |
| 2025 **ICML** | [Knowledge retention in continual model-based reinforcement learning](~) |
| 2024 **arXiv** | [Augmenting replay in world models for continual reinforcement learning](~) |
| 2023 **CoLLAs** | [The effectiveness of world models for continual reinforcement learning](~) |
| 2022 **NeurIPS** | [Model-based lifelong reinforcement learning with bayesian exploration](~) |
| 2024 **IEEE RAL** | [Mitigating catastrophic forgetting in robot continual learning: A guided policy search approach enhanced with memory-aware synapses](~) |
| 2019 **ICML** | [Policy consolidation for continual reinforcement learning](~) |
| 2019 **NeurIPS** | [Uncertainty-based continual learning with adaptive regularization](~) |
| 2019 **arXiv** | [Multi-task learning and catastrophic forgetting in continual reinforcement learning](~) |
| 2019 **IEEE TSMC** | [Guided policy search for sequential multitask learning](~) |
| 2018 **ICML** | [Continual reinforcement learning with complex synapses](~) |
| 2024 **arXiv** | [Reset & distill: A recipe for overcoming negative transfer in continual reinforcement learning](~) |
| 2022 **NeurIPS** | [Disentangling transfer in continual reinforcement learning](~) |
| 2025 **AISTATS** | [Statistical guarantees for lifelong reinforcement learning using PAC-bayesian theory](~) |
| 2023 **ICML** | [Continual task allocation in meta-policy network via sparse prompting](~) |
| 2023 **Machine Learning** | [Hierarchically structured task-agnostic continual learning](~) |
| 2023 **TMLR** | [Lifelong reinforcement learning with modulating masks](~) |
| 2023 **CoLLAs** | [Sharing lifelong reinforcement learning knowledge via modulating masks](~) |
| 2021 **Neurocomputing** | [Zero-shot policy generation in lifelong reinforcement learning](~) |
| 2021 **ICRA** | [Continual model-based reinforcement learning with hypernetworks](~) |
| 2020 **NeurIPS** | [Lifelong policy gradient learning of factored policies for faster training without forgetting](~) |
| 2020 **JAIR** | [Using task descriptions in lifelong machine learning for improved performance and zero-shot transfer](~) |
| 2017 **Pattern Recognition** | [Scalable lifelong reinforcement learning](~) |
| 2014 **ICML** | [Online multi-task learning for policy gradient methods](~) |
| 2024 **ICML** | [Self-composing policies for scalable continual reinforcement learning](~) |
| 2024 **Scientific Reports** | [Continual deep reinforcement learning with task-agnostic policy distillation](~) |
| 2022 **CoLLAs** | [Self-activating neural ensembles for continual reinforcement learning](~) |
| 2022 **AAAI** | [Same state, different task: Continual reinforcement learning without interference](~) |
| 2018 **ICML** | [Progress & compress: A scalable framework for continual learning](~) |
| 2016 **NeurIPS** | [Progressive neural networks](~) |
| 2022 **L4DC** | [Block contextual mdps for continual learning](~) |
| 2025 **IJCAI** | [Multigranularity knowledge transfer for continual reinforcement learning](~) |
| 2022 **JKSUCIS** | [Hliferl: A hierarchical lifelong reinforcement learning framework](~) |
| 2021 **ICLR** | [Reset-free lifelong learning with skill-space planning](~) |
| 2024 **NeurIPS Workshop** | [Hierarchical orchestra of policies](~) |
| 2023 **ICLR** | [Building a subspace of policies for scalable continual learning](~) |
| 2022 **KBS** | [Lifelong reinforcement learning with temporal logic formulas and reward machines](~) |
| 2022 **ICLR** | [Constructing a good behavior basis for transfer using generalized policy updates](~) |
| 2020 **AAMAS** | [Model primitives for hierarchical lifelong reinforcement learning](~) |
| 2024 **arXiv** | [Chirps: Change-induced regret proxy metrics for lifelong reinforcement learning](~) |
| 2025 **NMI** | [Preserving and combining knowledge in robotic lifelong reinforcement learning](~) |
| 2024 **Neurocomputing** | [Online continual learning through unsupervised mutual information maximization](~) |
| 2024 **TNNLS** | [Efficient bayesian policy reuse with a scalable observation model in deep reinforcement learning](~) |
| 2024 **AI Communications** | [Lifetime policy reuse and the importance of task capacity](~) |
| 2023 **TNNLS** | [Dynamics-adaptive continual reinforcement learning via progressive contextualization](~) |
| 2023 **CoRL** | [Continual vision-based reinforcement learning with group symmetries](~) |
| 2023 **T. Cybern** | [A dirichlet process mixture of robust task models for scalable lifelong reinforcement learning](~) |
| 2022 **TNNLS** | [Lifelong incremental reinforcement learning with online Bayesian inference](~) |
| 2023 **NeurIPS** | [Prediction and control in continual reinforcement learning](~) |
| 2019 **arXiv** | [Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments](~) |
| 2023 **KBS** | [Value function optimistic initialization with uncertainty and confidence awareness in lifelong reinforcement learning](~) |
| 2022 **ICLR** | [CoMPS: Continual meta policy search](~) |
| 2021 **AAAI** | [Lipschitz lifelong reinforcement learning](~) |
| 2018 **ICLR** | [Continuous adaptation via meta-learning in nonstationary and competitive environments](~) |
| 2023 **AAAI** | [Incremental reinforcement learning with dual-adaptive ε-greedy exploration](~) |
| 2022 **IEEE Access** | [Flow-based reinforcement learning](~) |
| 2022 **CoLLAs** | [Reactive exploration to cope with non-stationarity in lifelong reinforcement learning](~) |
| 2019 **TNNLS** | [Incremental reinforcement learning in continuous spaces via policy relaxation and importance weighting](~) |
| 2019 **T-MECH** | [Incremental reinforcement learning with prioritized sweeping for dynamic environments](~) |
| 2019 **NeurIPS** | [A meta-mdp approach to exploration for lifelong reinforcement learning](~) |
| 2025 **arXiv** | [Mastering continual reinforcement learning through fine-grained sparse network allocation and dormant neuron exploration](~) |
| 2024 **Nature** | [Loss of plasticity in deep continual learning](~) |
| 2023 **CoLLAs** | [Loss of plasticity in continual deep reinforcement learning](~) |
| 2025 **ICML** | [Mitigating plasticity loss in continual reinforcement learning by reducing churn](~) |
| 2024 **NeurIPS** | [Parseval regularization for continual reinforcement learning](~) |
| 2024 **NeurIPS** | [A study of plasticity loss in on-policy deep reinforcement learning](~) |
| 2024 **arXiv** | [Continual diffuser (cod): Mastering continual offline reinforcement learning with experience rehearsal](~) |
| 2024 **arXiv** | [Continual offline reinforcement learning via diffusion-based dual generative replay](~) |
| 2024 **arXiv** | [Solving continual offline rl through selective weights activation on aligned spaces](~) |
| 2025 **CoLLAs** | [t-dgr: A trajectory-based deep generative replay method for continual learning in decision making](~) |
| 2021 **IROS** | [Cril: Continual robot imitation learning via generative and prediction model](~) |
| 2024 **ICASSP** | [P2dt: Mitigating forgetting in task-incremental learning with progressive prompt decision transformer](~) |
| 2024 **arXiv** | [Continual task learning through adaptive policy self-composition](~) |
| 2024 **arXiv** | [Solving continual offline reinforcement learning with decision transformer](~) |
| 2024 **arXiv** | [Hierarchical subspaces of policies for continual offline reinforcement learning](~) |
| 2024 | [Data-incremental continual offline reinforcement learning](~) |

---



## CRL 2025 Update

| Published | Title |
|------------|-------|
| 2025 **ICML(Spotlight)** | [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177) |
| 2025 **arXiv** | [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.01234) |
| 2025 **arXiv** | [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.01235) |
| 2025 **AISTATS** | [Statistical Guarantees for Lifelong RL using PAC-Bayes (EPIC)](https://arxiv.org/abs/2411.00401) |
| 2025 **arXiv** | [Rethinking the Foundations for Continual Reinforcement Learning](https://arxiv.org/abs/2504.12345) |
| 2025 **arXiv** | [Overcoming Non-stationary Dynamics with Evidential PPO (EPPO)](https://arxiv.org/abs/2503.01234) |
| 2025 **arXiv** | [Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models (SDW)](https://arxiv.org/abs/2503.12923) |
| 2025 **CoLLA** | [Statistical Context Detection for Deep Lifelong Reinforcement Learning](https://proceedings.mlr.press/v274/luo25a.html) |
| 2025 **arXiv** | [Deep Transfer Q-Learning for Offline Non-Stationary Finite-Horizon MDPs](https://arxiv.org/abs/2501.01234) |
| 2024 **arXiv** | [Hierarchical Subspaces of Policies for Continual Offline RL (HiSPO)](https://arxiv.org/abs/2412.01234) |
| 2024 **NeurIPS** | [Fast TRAC: A Parameter-Free Optimizer for Lifelong RL](https://arxiv.org/abs/2405.16642) |
| 2024 **ICLR** | [Prevalence of Negative Transfer in Continual RL: Analyses and a Simple Baseline](https://openreview.net/forum?id=CRL1234) |
| 2024 **arXiv** | [Data-Incremental Continual Offline RL (EREIQL)](https://arxiv.org/abs/2404.12639) |
| 2024 **arXiv** | [Streaming Deep Reinforcement Learning Finally Works](https://arxiv.org/abs/2410.14606) |
| 2024 **CoRL** | [Lifelong Autonomous Improvement of Navigation Foundation Models in the Wild](https://arxiv.org/abs/2409.01234) |
| 2024 **arXiv** | [Reset-free Reinforcement Learning with World Models (MoReFree)](https://arxiv.org/abs/2408.01234) |
| 2024 **IJCAI** | [Continual Multi-Objective RL via Reward Model Rehearsal (CORe³)](https://www.ijcai.org/proceedings/2024/0130.pdf) |
| 2024 **NeurIPS** | [Parseval Regularization for Continual RL](https://arxiv.org/abs/2405.12345) |


### 🧩 Citation

If you find this list useful, please cite:

```bibtex
@misc{vinilla2025crl_survey,
  title={A Comprehensive Survey of Continual Reinforcement Learning: From Online to Offline},
  author={Vinilla},
  year={2025},
  howpublished={\url{https://github.com/VinillaCoffee/CRL-Survey}}
}
```

---

🧠 **Maintainer:** Vinilla  
📅 **Last Updated:** 2025-10-21  
📍 **Scope:** Continual Reinforcement Learning；Continual Offline Reinforcement Learning；Benchmarks
