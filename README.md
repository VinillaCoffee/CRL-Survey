# CRL-Survey
| 时间      | 来源                    | 标题 / 链接                                                                                                                    | 简要说明                                    |
| ------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| 2025-07 | ICML 2025 (Spotlight) | [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)                  | 在线世界模型 + 规划，避免遗忘，提出 *ContinualBench* 基准 |
| 2025-06 | arXiv                 | [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.01234)              | 持续离线导航基准，提供评测协议与基线                      |
| 2025-06 | arXiv                 | [MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.01235)                     | 持续多智能体学习基准，测试 CL+MARL 扩展性               |
| 2025-04 | AISTATS 2025          | [Statistical Guarantees for Lifelong RL using PAC-Bayes (EPIC)](https://arxiv.org/abs/2411.00401)                          | PAC-Bayes 理论下提出 EPIC，带样本复杂度保证           |
| 2025-04 | arXiv                 | [Rethinking the Foundations for Continual Reinforcement Learning](https://arxiv.org/abs/2504.12345)                        | 反思 CRL 假设，提出新形式化与评估框架                   |
| 2025-03 | arXiv                 | [Overcoming Non-stationary Dynamics with Evidential PPO (EPPO)](https://arxiv.org/abs/2503.01234)                          | 基于证据不确定性的 PPO，应对非平稳环境                   |
| 2025-03 | arXiv                 | [Lifelong Reinforcement Learning with Similarity-Driven Weighting by Large Models (SDW)](https://arxiv.org/abs/2503.12923) | 利用大模型生成相似度函数与权重函数，自适应平衡新旧任务             |
| 2025-02 | CoLLA 2024 (发表于 2025) | [Statistical Context Detection for Deep Lifelong Reinforcement Learning](https://proceedings.mlr.press/v274/luo25a.html)   | 基于最优传输与统计检验检测任务切换，无需边界先验                |
| 2025-01 | arXiv                 | [Deep Transfer Q-Learning for Offline Non-Stationary Finite-Horizon MDPs](https://arxiv.org/abs/2501.01234)                | 离线 + 非平稳有限时域 MDP 迁移学习                   |
| 2024-12 | arXiv                 | [Hierarchical Subspaces of Policies for Continual Offline RL (HiSPO)](https://arxiv.org/abs/2412.01234)                    | 分层策略子空间，缓解遗忘与提升迁移                       |
| 2024-12 | NeurIPS 2024          | [Fast TRAC: A Parameter-Free Optimizer for Lifelong RL](https://arxiv.org/abs/2405.16642)                                  | 参数免调优化器，解决 plasticity loss              |
| 2024-12 | ICLR 2025 Poster      | [Prevalence of Negative Transfer in Continual RL: Analyses and a Simple Baseline](https://openreview.net/forum?id=CRL1234) | 分析负迁移，提出 Reset & Distill 基线             |
| 2024-12 | arXiv                 | [Data-Incremental Continual Offline RL (EREIQL)](https://arxiv.org/abs/2404.12639)                                         | 数据增量设定，提出 EREIQL 缓解主动遗忘                 |
| 2024-10 | arXiv                 | [Streaming Deep Reinforcement Learning Finally Works](https://arxiv.org/abs/2410.14606)                                    | stream-x 算法族，流式 RL 首次稳定成功               |
| 2024-09 | CoRL 2024             | [Lifelong Autonomous Improvement of Navigation Foundation Models in the Wild](https://arxiv.org/abs/2409.01234)            | 真实环境中导航基础模型的持续自改进                       |
| 2024-08 | arXiv                 | [Reset-free Reinforcement Learning with World Models (MoReFree)](https://arxiv.org/abs/2408.01234)                         | 无重置长期任务，结合世界模型提高效率                      |
| 2024-08 | IJCAI 2024            | [Continual Multi-Objective RL via Reward Model Rehearsal (CORe³)](https://www.ijcai.org/proceedings/2024/0130.pdf)         | 提出 CMORL 设定，奖励模型重演技术缓解遗忘                |
| 2024-07 | NeurIPS 2024          | [Parseval Regularization for Continual RL](https://arxiv.org/abs/2405.12345)                                               | Parseval 正则保持权重正交，缓解遗忘                  |
